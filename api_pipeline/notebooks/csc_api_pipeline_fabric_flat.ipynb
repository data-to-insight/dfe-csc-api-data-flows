{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03afbea6",
   "metadata": {},
   "source": [
    "\n",
    "# CSC API Pipeline (Flattened for Fabric)\n",
    "\n",
    "This notebook contains the full flattened logic of the CSC API CLI version:\n",
    "- Config loading and environment setup\n",
    "- SQL Server connectivity\n",
    "- OAuth2 token authentication\n",
    "- JSON payload generation (full or partial mode)\n",
    "- API submission and response handling\n",
    "- Optional diagnostics (DB test, schema check, dry-run diff and smoke_test as run_smoke() <-- this set to run by default)\n",
    "\n",
    "This entire notebook(all cells) can be safely run out of the box with the existing default settings. \n",
    "Only the fake payload smoke-test will run until # main() in section 'Run Submission' is uncommented.  \n",
    "\n",
    "> Designed for standalone use in Microsoft Fabric (no external .py dependencies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c330a24",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "File: `config.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f58bac4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded environment from: env.txt\n",
      "[config] DEBUG = True\n",
      "[config] USE_PARTIAL_PAYLOAD = True\n",
      "[config] TABLE_NAME = ssd_api_data_staging_anon\n"
     ]
    }
   ],
   "source": [
    "# api_pipeline/config.py\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Dev\n",
    "# Try .env first, fallback to env.txt\n",
    "# Robust load: notebook and CLI\n",
    "for candidate in [\".env\", \"env.txt\"]:\n",
    "    env_path = Path(candidate)\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"[config] Loaded environment from: {env_path}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"[config] No .env or env.txt found – using system environment variables only\")\n",
    "\n",
    "# Config flags\n",
    "DEBUG = os.getenv(\"DEBUG\", \"false\").strip().lower() == \"true\"\n",
    "USE_PARTIAL_PAYLOAD = os.getenv(\"USE_PARTIAL_PAYLOAD\", \"true\").strip().lower() == \"true\"\n",
    "TABLE_NAME = os.getenv(\"TABLE_NAME\", \"ssd_api_data_staging_anon\")\n",
    "\n",
    "# debug current mode\n",
    "print(f\"[config] DEBUG = {DEBUG}\")\n",
    "print(f\"[config] USE_PARTIAL_PAYLOAD = {USE_PARTIAL_PAYLOAD}\")\n",
    "print(f\"[config] TABLE_NAME = {TABLE_NAME}\")\n",
    "\n",
    "      \n",
    "    \n",
    "\n",
    "# --- Required individual components ---\n",
    "USER_SERVER = os.getenv(\"USER_SERVER\")\n",
    "USER_DATABASE = os.getenv(\"USER_DATABASE\")\n",
    "API_ENDPOINT = os.getenv(\"API_ENDPOINT\")\n",
    "LA_CODE = os.getenv(\"LA_CODE\")\n",
    "\n",
    "\n",
    "# --- Derived vals ---\n",
    "# fallback: try SQL_CONN_STR if present, else build from parts\n",
    "_sql_conn_str_env = os.getenv(\"SQL_CONN_STR\")\n",
    "if _sql_conn_str_env:\n",
    "    SQL_CONN_STR = _sql_conn_str_env\n",
    "else:\n",
    "    SQL_CONN_STR = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={USER_SERVER};DATABASE={USER_DATABASE};Trusted_Connection=yes;\"\n",
    "\n",
    "API_ENDPOINT_LA = f\"{API_ENDPOINT}/children_social_care_data/{LA_CODE}/children\"\n",
    "\n",
    "# --- Other config ---\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "SCOPE = os.getenv(\"SCOPE\")\n",
    "TOKEN_ENDPOINT = os.getenv(\"TOKEN_ENDPOINT\")\n",
    "SUPPLIER_KEY = os.getenv(\"SUPPLIER_KEY\")\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 100))\n",
    "\n",
    "\n",
    "\n",
    "# # ----------- Config OVERIDE -----------\n",
    "# # D2I Overide block\n",
    "\n",
    "# # Within 'Subscription details' block, ensure that your 'Status' has been set to 'Active' by the DfE\n",
    "\n",
    "# # https://pp-find-and-use-an-api.education.gov.uk/\n",
    "# $api_endpoint = \"https://pp-api.education.gov.uk/children-in-social-care-data-receiver-test/1\" # 'Base URL'\n",
    "\n",
    "\n",
    "# # From the 'subscription key' block\n",
    "# $supplier_key = \"6736ad89172548dcaa3529896892ab3f\" # 'Primary key' or 'Secondary key'\n",
    "\n",
    "# # From the 'Native OAuth Application-flow' block\n",
    "# $token_endpoint = \"https://login.microsoftonline.com/cc0e4d98-b9f9-4d58-821f-973eb69f19b7/oauth2/v2.0/token\" # 'OAuth token endpoint'\n",
    "# $client_id = \"fe28c5a9-ea4f-4347-b419-189eb761fa42\"  # 'OAuth Client ID'\n",
    "# $client_secret = \"mR_8Q~G~Wdm2XQ2E8-_hr0fS5FKEns4wHNLtbdw7\" # 'Primary key' or 'Secondary key'\n",
    "# $scope = \"api://children-in-social-care-data-receiver-test-1-live_6b1907e1-e633-497d-ac74-155ab528bc17/.default\" # 'OAuth Scope'\n",
    "\n",
    "# $la_code        = 845   # e.g. 846 etc.\n",
    "# # ----------- Config OVERIDE END -----------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Enforced json structure elements\n",
    "REQUIRED_FIELDS = [\n",
    "    \"la_child_id\",\n",
    "    \"mis_child_id\",\n",
    "    \"child_details\"\n",
    "]\n",
    "\n",
    "ALLOWED_PURGE_BLOCKS = [\n",
    "    \"social_care_episodes\",\n",
    "    \"child_protection_plans\",\n",
    "    \"child_in_need_plans\",\n",
    "    \"health_and_wellbeing\",\n",
    "    \"care_leavers\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# --- Env var validation helper ---\n",
    "def validate_env_vars(required_vars):\n",
    "    missing = [var for var in required_vars if not os.getenv(var)]\n",
    "    if missing:\n",
    "        raise EnvironmentError(f\"Missing required environment variables: {', '.join(missing)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014df651",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "File: `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "786df8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_pipeline/utils.py\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "import pyodbc  # for notebook/test_db_connection snippet(end block)\n",
    "\n",
    "# --- Optional memory_profiler support ----------------------------------------\n",
    "# If memory_profiler not installed (e.g., in PyInstaller EXE), degrade\n",
    "# gracefully and omit \"Mem delta\" from benchmark output\n",
    "try:\n",
    "    from memory_profiler import memory_usage as _memory_usage\n",
    "except Exception:\n",
    "    _memory_usage = None\n",
    "\n",
    "def _safe_memory_usage_first_sample():\n",
    "    \"\"\"\n",
    "    Returns the first memory usage sample in MiB if memory_profiler is available,\n",
    "    otherwise returns None. Never raises.\n",
    "    \"\"\"\n",
    "    if _memory_usage is None:\n",
    "        return None\n",
    "    try:\n",
    "        samples = _memory_usage()\n",
    "        return samples[0] if samples else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# # --- CLI / .exe fallback config (not needed in notebook)\n",
    "# try:\n",
    "#     # running as proper package\n",
    "#     from .config import DEBUG, USE_PARTIAL_PAYLOAD\n",
    "# except ImportError:\n",
    "#     try:\n",
    "#         # Fallback -running loose scripts\n",
    "#         from config import DEBUG, USE_PARTIAL_PAYLOAD\n",
    "#     except Exception:\n",
    "#         # Last‑ditch -defaults avoid import‑time crashes in --help or EXE smoke tests\n",
    "#         DEBUG = False\n",
    "#         USE_PARTIAL_PAYLOAD = False\n",
    "\n",
    "\n",
    "\n",
    "# --- Logging / mode helpers ---------------------------------------------------\n",
    "def log_debug(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "def announce_mode():\n",
    "    print(\"Running in development mode\" if DEBUG else \"▶ Running in production mode\")\n",
    "    print(\"Partial delta payload mode enabled\" if USE_PARTIAL_PAYLOAD else \"▶ Full non-delta payload mode only\")\n",
    "\n",
    "# --- Benchmark decorator ------------------------------------------------------\n",
    "def benchmark_section(label: str):\n",
    "    \"\"\"\n",
    "    Decorator to benchmark a function section.\n",
    "    - Always measures elapsed time and tracemalloc peak memory.\n",
    "    - If memory_profiler is available, also reports process \"Mem delta\" in MiB.\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            print(f\"Starting {label}...\")\n",
    "            start = time.time()\n",
    "            tracemalloc.start()\n",
    "\n",
    "            mem_before = _safe_memory_usage_first_sample()\n",
    "\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            mem_after = _safe_memory_usage_first_sample()\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "\n",
    "            if DEBUG:\n",
    "                peak_mib = peak / (1024 ** 2)\n",
    "                if mem_before is not None and mem_after is not None:\n",
    "                    mem_delta = mem_after - mem_before\n",
    "                    print(f\"Finished {label}: {elapsed:.2f}s | Mem delta: {mem_delta:.2f} MiB | Peak mem: {peak_mib:.2f} MiB\")\n",
    "                else:\n",
    "                    # memory_profiler not present (or failed): omit Mem delta\n",
    "                    print(f\"Finished {label}: {elapsed:.2f}s | Peak mem: {peak_mib:.2f} MiB\")\n",
    "\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# # Test DB connection\n",
    "# # drop-in block to .ipynb or main()\n",
    "# import sys\n",
    "# from .config import SQL_CONN_STR  # or: from config import SQL_CONN_STR\n",
    "# from utils import test_db_connection\n",
    "# if not test_db_connection(SQL_CONN_STR):\n",
    "#     sys.exit(1)  # Stop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bdb5eb",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "File: `auth.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94cb4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_pipeline/auth.py\n",
    "import requests\n",
    "\n",
    "# from .config import CLIENT_ID, CLIENT_SECRET, SCOPE, TOKEN_ENDPOINT\n",
    "# from .utils import log_debug\n",
    "\n",
    "# # ---- local imports with fallback for notebook/debug use ----\n",
    "# try:\n",
    "#     from .config import CLIENT_ID, CLIENT_SECRET, SCOPE, TOKEN_ENDPOINT\n",
    "#     from .utils import log_debug\n",
    "# except ImportError:\n",
    "#     from config import CLIENT_ID, CLIENT_SECRET, SCOPE, TOKEN_ENDPOINT\n",
    "#     from utils import log_debug\n",
    "\n",
    "\n",
    "def get_oauth_token():\n",
    "    payload = {\n",
    "        \"client_id\": CLIENT_ID,\n",
    "        \"client_secret\": CLIENT_SECRET,\n",
    "        \"scope\": SCOPE,\n",
    "        \"grant_type\": \"client_credentials\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(TOKEN_ENDPOINT, data=payload)\n",
    "        response.raise_for_status()\n",
    "        token = response.json()[\"access_token\"]\n",
    "        print(\"OAuth token retrieved.\")\n",
    "        \n",
    "        log_debug(f\"TOKEN (first 10 chars): {token[:10]}...\")\n",
    "        \n",
    "        return token\n",
    "    except Exception as e:\n",
    "        print(f\"OAuth Token Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb435c8b",
   "metadata": {},
   "source": [
    "## Database Access\n",
    "File: `db.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8019e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# from .config import TABLE_NAME, USE_PARTIAL_PAYLOAD\n",
    "# from .payload import generate_partial_payload, generate_deletion_payload\n",
    "# from .utils import benchmark_section\n",
    "\n",
    "# # ---- local imports with fallback for notebook/debug use ----\n",
    "# try:\n",
    "#     from .config import TABLE_NAME, USE_PARTIAL_PAYLOAD\n",
    "#     from .payload import generate_partial_payload, generate_deletion_payload\n",
    "#     from .utils import benchmark_section\n",
    "# except ImportError:\n",
    "#     from config import TABLE_NAME, USE_PARTIAL_PAYLOAD\n",
    "#     from payload import generate_partial_payload, generate_deletion_payload\n",
    "#     from utils import benchmark_section\n",
    "\n",
    "\n",
    "# ---- DATA ----\n",
    "# PEP 484 signature:\n",
    "# def update_partial_payloads(conn: pyodbc.Connection) -> None:\n",
    "@benchmark_section(\"update_partial_payloads()\")\n",
    "def update_partial_payloads(conn):\n",
    "    \"\"\"\n",
    "    Update table with generated partial JSON payloads\n",
    "\n",
    "    Args:\n",
    "        conn: Open database connection\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Select rows with both current, previous JSON\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT person_id, row_state, json_payload, previous_json_payload \n",
    "        FROM {TABLE_NAME}\n",
    "        WHERE \n",
    "            json_payload IS NOT NULL\n",
    "            AND previous_json_payload IS NOT NULL\n",
    "            AND row_state <> 'unchanged'\n",
    "    \"\"\")\n",
    "\n",
    "    updates = []\n",
    "\n",
    "    # ---- DIAGNOSTIC COUNTERS ----\n",
    "    total_checked = 0\n",
    "    skipped_due_to_state = 0\n",
    "    skipped_due_to_equal_json = 0\n",
    "    new_record_count = 0\n",
    "    deletion_count = 0\n",
    "    delta_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    # ---------------------------------------\n",
    "\n",
    "    for person_id, row_state, curr_raw, prev_raw in cursor.fetchall():\n",
    "        total_checked += 1\n",
    "        try:\n",
    "            # ---- EARLY EXIT: skip if new or unchanged ----\n",
    "            if row_state.lower() == \"unchanged\":\n",
    "                skipped_due_to_state += 1\n",
    "                continue\n",
    "\n",
    "            # ---- EARLY EXIT: skip if identical JSON strings ----\n",
    "            if curr_raw == prev_raw:\n",
    "                skipped_due_to_equal_json += 1\n",
    "                continue\n",
    "\n",
    "            curr = json.loads(curr_raw)  # Parse current JSON\n",
    "            prev = json.loads(prev_raw)  # Parse previous JSON\n",
    "\n",
    "            # Generate appropriate payload by row state\n",
    "            if row_state.lower() == \"deleted\":\n",
    "                partial = generate_deletion_payload(prev)\n",
    "                deletion_count += 1\n",
    "            else:\n",
    "                partial = generate_partial_payload(curr, prev)\n",
    "                delta_count += 1\n",
    "\n",
    "            # Serialise partial and escape quotes for SQL\n",
    "            json_out = json.dumps(partial, separators=(',', ':'), ensure_ascii=False).replace(\"'\", \"''\")\n",
    "            updates.append((json_out, person_id))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {person_id}: {e}\")\n",
    "            error_count += 1\n",
    "\n",
    "    # Apply updates to db\n",
    "    for json_out, pid in updates:\n",
    "        cursor.execute(f\"\"\"\n",
    "            UPDATE {TABLE_NAME}\n",
    "            SET partial_json_payload = ?\n",
    "            WHERE person_id = ?\n",
    "        \"\"\", json_out, pid)\n",
    "\n",
    "    conn.commit()\n",
    "    print(f\"Updated {len(updates)} partial_json_payload records\")\n",
    "\n",
    "    # ---- DIAGS OUTPUT ----\n",
    "    print(f\"[DIAG] Checked: {total_checked}\")\n",
    "    print(f\"[DIAG] Skipped (state): {skipped_due_to_state}\")\n",
    "    print(f\"[DIAG] Skipped (identical JSON): {skipped_due_to_equal_json}\")\n",
    "    print(f\"[DIAG] Deleted payloads: {deletion_count}\")\n",
    "    print(f\"[DIAG] Delta payloads: {delta_count}\")\n",
    "    print(f\"[DIAG] Errors: {error_count}\")\n",
    "    # --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# PEP 484 signature:\n",
    "# def get_pending_records(cursor: pyodbc.Cursor) -> List[Dict[str, Any]]:\n",
    "@benchmark_section(\"get_pending_records()\")  # Performance monitor\n",
    "def get_pending_records(cursor):\n",
    "    \"\"\"\n",
    "    Fetch pending/error records with non-empty payload\n",
    "\n",
    "    Args:\n",
    "        cursor: Active database cursor\n",
    "\n",
    "    Returns:\n",
    "        List of records with parsed JSON payload.\n",
    "    \"\"\"\n",
    "    col = \"partial_json_payload\" if USE_PARTIAL_PAYLOAD else \"json_payload\"\n",
    "\n",
    "    # Select valid rows by status and payload\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT person_id, {col}\n",
    "        FROM {TABLE_NAME}\n",
    "        WHERE submission_status IN ('pending', 'error')\n",
    "        AND {col} IS NOT NULL AND LTRIM(RTRIM({col})) <> ''\n",
    "    \"\"\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for pid, payload in cursor.fetchall():\n",
    "        try:\n",
    "            results.append({\n",
    "                \"person_id\": pid,\n",
    "                \"json\": json.loads(payload)  # Parse JSON safely\n",
    "            })\n",
    "        except:\n",
    "            print(f\"Skipping invalid JSON for person_id {pid}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# ---- DB UPDATES ----\n",
    "# PEP 484 signature:\n",
    "# def update_api_success(cursor: pyodbc.Cursor, person_id: str, uuid: str, timestamp: str) -> None:\n",
    "@benchmark_section(\"update_api_success()\")  # Performance monitor\n",
    "def update_api_success(cursor, person_id, uuid, timestamp):\n",
    "    \"\"\"\n",
    "    Mark record as sent with API response and timestamp\n",
    "\n",
    "    Args:\n",
    "        cursor: Active database cursor\n",
    "        person_id: Person identifier\n",
    "        uuid: API response reference\n",
    "        timestamp: Submission timestamp\n",
    "    \"\"\"\n",
    "    cursor.execute(f\"\"\"\n",
    "        UPDATE {TABLE_NAME}\n",
    "        SET submission_status='sent',\n",
    "            api_response=?,\n",
    "            submission_timestamp=?,\n",
    "            previous_hash=current_hash,\n",
    "            previous_json_payload=json_payload,\n",
    "            row_state='unchanged'\n",
    "        WHERE person_id = ?\n",
    "    \"\"\", uuid, timestamp, person_id)\n",
    "\n",
    "\n",
    "\n",
    "# PEP 484 signature:\n",
    "# def update_api_failure(cursor: pyodbc.Cursor, person_id: str, message: str) -> None:\n",
    "@benchmark_section(\"update_api_failure()\")  # Performance monitor\n",
    "def update_api_failure(cursor, person_id, message):\n",
    "    \"\"\"\n",
    "    Mark record as failed, store API error message\n",
    "\n",
    "    Args:\n",
    "        cursor: Active database cursor\n",
    "        person_id: Person identifier\n",
    "        message: Error message from API or client\n",
    "    \"\"\"\n",
    "    cursor.execute(f\"\"\"\n",
    "        UPDATE {TABLE_NAME}\n",
    "        SET submission_status='error',\n",
    "            api_response=?\n",
    "        WHERE person_id = ?\n",
    "    \"\"\", message[:500], person_id)  # Truncate to max allowed size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef349230",
   "metadata": {},
   "source": [
    "## Payload Builder\n",
    "File: `payload.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3fedde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_pipeline/payload.py\n",
    "\n",
    "import json\n",
    "\n",
    "# from .config import REQUIRED_FIELDS, ALLOWED_PURGE_BLOCKS\n",
    "# from .utils import benchmark_section\n",
    "\n",
    "# # ---- local imports with fallback for notebook/debug use ----\n",
    "# try:\n",
    "#     from .config import REQUIRED_FIELDS, ALLOWED_PURGE_BLOCKS\n",
    "#     from .utils import benchmark_section\n",
    "# except ImportError:\n",
    "#     from config import REQUIRED_FIELDS, ALLOWED_PURGE_BLOCKS\n",
    "#     from utils import benchmark_section\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [DIAG] Add diagnostics\n",
    "from time import perf_counter  # [DIAG]\n",
    "_recursive_diff_call_count = 0  # [DIAG]\n",
    "_recursive_diff_total_time = 0  # [DIAG]\n",
    "\n",
    "_prune_call_count = 0  # [DIAG]\n",
    "_prune_total_time = 0  # [DIAG]\n",
    "\n",
    "\n",
    "@benchmark_section(\"generate_partial_payload()\")  # Performance monitor\n",
    "def generate_partial_payload(current, previous):\n",
    "    \"\"\"\n",
    "    Create partial payload with required and changed fields\n",
    "\n",
    "    Args:\n",
    "        current: Current full JSON payload\n",
    "        previous: Previous full JSON payload\n",
    "\n",
    "    Returns:\n",
    "        Partial payload with required identifiers and detected changes\n",
    "    \"\"\"\n",
    "    # Copy required fields\n",
    "    partial = {k: current[k] for k in REQUIRED_FIELDS if k in current}\n",
    "\n",
    "    # Compute structural differences\n",
    "    diff = recursive_diff(current, previous)\n",
    "\n",
    "    # Add new fields not already present\n",
    "    for k, v in diff.items():\n",
    "        if k not in partial:\n",
    "            partial[k] = v\n",
    "\n",
    "    return partial\n",
    "\n",
    "\n",
    "@benchmark_section(\"generate_deletion_payload()\")  # Performance monitor\n",
    "def generate_deletion_payload(previous):\n",
    "    \"\"\"\n",
    "    Create deletion payload using identifiers and purge flag\n",
    "\n",
    "    Args:\n",
    "        previous: Previous full JSON payload\n",
    "\n",
    "    Returns:\n",
    "        Minimal payload with identifiers and deletion marker\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"la_child_id\": previous.get(\"la_child_id\"),  # Unique child identifier\n",
    "        \"mis_child_id\": previous.get(\"mis_child_id\"),  # MIS identifier\n",
    "        \"purge\": True  # Purge signal for deletion\n",
    "    }\n",
    "\n",
    "\n",
    "@benchmark_section(\"recursive_diff()\")  # Performance monitor\n",
    "def recursive_diff(curr, prev):\n",
    "    # [DIAG] Start timing and count\n",
    "    global _recursive_diff_call_count, _recursive_diff_total_time  # [DIAG]\n",
    "    _recursive_diff_call_count += 1  # [DIAG]\n",
    "    _start = perf_counter()  # [DIAG]\n",
    "\n",
    "    if isinstance(curr, dict) and isinstance(prev, dict):\n",
    "        diff = {}\n",
    "\n",
    "        for key in curr:\n",
    "            # Skip if key unchanged\n",
    "            if key not in prev or curr[key] != prev[key]:\n",
    "\n",
    "                # Recurse into nested dicts\n",
    "                if isinstance(curr[key], dict) and isinstance(prev.get(key), dict):\n",
    "                    nested = recursive_diff(curr[key], prev[key])\n",
    "                    if nested:\n",
    "                        diff[key] = nested  # Include only if changes present\n",
    "\n",
    "                # Recurse into lists\n",
    "                elif isinstance(curr[key], list) and isinstance(prev.get(key), list):\n",
    "                    if curr[key] != prev[key]:\n",
    "                        # Pass parent key to control 'purge' inclusion\n",
    "                        diff[key] = prune_unchanged_list(\n",
    "                            curr[key], prev[key], parent_key=key\n",
    "                        )\n",
    "\n",
    "                # Handle scalars or mismatched types\n",
    "                else:\n",
    "                    diff[key] = curr[key]\n",
    "\n",
    "        _recursive_diff_total_time += perf_counter() - _start  # [DIAG]\n",
    "        return diff  # Return dict of differences\n",
    "\n",
    "    # Return scalar diff, or empty if no change\n",
    "    result = {} if curr == prev else curr\n",
    "    _recursive_diff_total_time += perf_counter() - _start  # [DIAG]\n",
    "    return result\n",
    "\n",
    "\n",
    "@benchmark_section(\"prune_unchanged_list()\")  # Performance monitor\n",
    "def prune_unchanged_list(curr_list, prev_list, parent_key=None):\n",
    "    # [DIAG] Start timing and count\n",
    "    global _prune_call_count, _prune_total_time  # [DIAG]\n",
    "    _prune_call_count += 1  # [DIAG]\n",
    "    _start = perf_counter()  # [DIAG]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for curr_item in curr_list:\n",
    "        matched_prev = None\n",
    "\n",
    "        # Detect ID key\n",
    "        id_key = next((k for k in curr_item if k.endswith(\"_id\")), None)\n",
    "\n",
    "        if id_key:\n",
    "            # Find match in previous by ID\n",
    "            for prev_item in prev_list:\n",
    "                if prev_item.get(id_key) == curr_item.get(id_key):\n",
    "                    matched_prev = prev_item\n",
    "                    break\n",
    "\n",
    "        if matched_prev:\n",
    "            # Diff current item against matched previous\n",
    "            item_diff = recursive_diff(curr_item, matched_prev)\n",
    "\n",
    "            # Always retain ID key\n",
    "            if id_key:\n",
    "                item_diff[id_key] = curr_item[id_key]\n",
    "\n",
    "            if item_diff:\n",
    "                # Set 'purge' flag only for allowed blocks\n",
    "                if parent_key in ALLOWED_PURGE_BLOCKS:\n",
    "                    item_diff[\"purge\"] = False\n",
    "                result.append(item_diff)\n",
    "\n",
    "        else:\n",
    "            # Unmatched: treat as new item\n",
    "            result.append(curr_item)\n",
    "\n",
    "    _prune_total_time += perf_counter() - _start  # [DIAG]\n",
    "    return result\n",
    "\n",
    "\n",
    "# [DIAG] summarise usage and timings\n",
    "def print_diff_stats():\n",
    "    print(f\"\\n[DIAG] recursive_diff() calls: {_recursive_diff_call_count}\")\n",
    "    print(f\"[DIAG] Total time in recursive_diff(): {_recursive_diff_total_time:.2f}s\")\n",
    "    if _recursive_diff_call_count:\n",
    "        print(f\"[DIAG] Avg time per recursive_diff(): {_recursive_diff_total_time / _recursive_diff_call_count:.6f}s\")\n",
    "\n",
    "    print(f\"\\n[DIAG] prune_unchanged_list() calls: {_prune_call_count}\")\n",
    "    print(f\"[DIAG] Total time in prune_unchanged_list(): {_prune_total_time:.2f}s\")\n",
    "    if _prune_call_count:\n",
    "        print(f\"[DIAG] Avg time per prune_unchanged_list(): {_prune_total_time / _prune_call_count:.6f}s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97723b77",
   "metadata": {},
   "source": [
    "## API Submission\n",
    "File: `api.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa3d4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_pipeline/api.py\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "# from .config import BATCH_SIZE, API_ENDPOINT_LA\n",
    "# from .db import update_api_success, update_api_failure\n",
    "# from .utils import benchmark_section, log_debug\n",
    "\n",
    "\n",
    "# # ---- local imports with fallback for notebook/debug use ----\n",
    "# try:\n",
    "#     from .config import BATCH_SIZE, API_ENDPOINT_LA\n",
    "#     from .db import update_api_success, update_api_failure\n",
    "#     from .utils import benchmark_section, log_debug\n",
    "# except ImportError:\n",
    "#     from config import BATCH_SIZE, API_ENDPOINT_LA\n",
    "#     from db import update_api_success, update_api_failure\n",
    "#     from utils import benchmark_section, log_debug\n",
    "\n",
    "\n",
    "# ---- API ----\n",
    "# PEP 484 signature:\n",
    "# def process_batches(records: List[Dict[str, Any]], headers: Dict[str, str], conn: pyodbc.Connection, max_retries: int = 3) -> None:\n",
    "@benchmark_section(\"process_batches()\")  # Performance monitor\n",
    "def process_batches(records, headers, conn, max_retries=3):\n",
    "    \"\"\"\n",
    "    Submit payloads in batches to API with retry logic\n",
    "\n",
    "    Args:\n",
    "        records: List of dicts with 'person_id' and parsed JSON\n",
    "        headers: HTTP headers for API call\n",
    "        conn: Open database connection\n",
    "        max_retries: Retry count before marking as failure\n",
    "    \"\"\"\n",
    "    total = len(records)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for i in range(0, total, BATCH_SIZE):\n",
    "        batch = records[i:i + BATCH_SIZE]\n",
    "\n",
    "\n",
    "        log_debug(f\"Processing batch {i + 1} to {i + len(batch)} of {total}\")  # DEBUG\n",
    "        \n",
    "        payload = [r[\"json\"] for r in batch]\n",
    "        payload_str = json.dumps(payload)\n",
    "\n",
    "        retries = 0\n",
    "        retry_delay = 5  # Seconds\n",
    "\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                # Submit batch to API\n",
    "                resp = requests.post(API_ENDPOINT_LA, headers=headers, data=payload_str)\n",
    "                raw_text = resp.text.strip()\n",
    "\n",
    "                if resp.status_code == 200:\n",
    "                    try:\n",
    "                        # Parse JSON response\n",
    "                        response_items = json.loads(raw_text)\n",
    "                        if not isinstance(response_items, list):\n",
    "                            print(\"Invalid API response: expected list, got\", type(response_items).__name__)\n",
    "                            for rec in batch:\n",
    "                                update_api_failure(cursor, rec[\"person_id\"], \"Invalid JSON structure from API\")\n",
    "                            conn.commit()\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        # Response invalid or unreadable\n",
    "                        print(\"Failed to parse JSON response. Logging all as failed.\")\n",
    "                        for rec in batch:\n",
    "                            update_api_failure(cursor, rec[\"person_id\"], f\"Invalid JSON response: {raw_text}\")\n",
    "                        conn.commit()\n",
    "                        break\n",
    "\n",
    "                    if len(response_items) == len(batch):\n",
    "                        for rec, item in zip(batch, response_items):\n",
    "                            try:\n",
    "                                # Parse UUID and timestamp\n",
    "                                date_part, time_part, uuid = item.split(\"_\")\n",
    "                                timestamp = datetime.strptime(f\"{date_part} {time_part}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                            except Exception:\n",
    "                                # Fallback to partial match and current time\n",
    "                                uuid = item.split(\"_\")[-1]\n",
    "                                timestamp = datetime.now()\n",
    "\n",
    "                            update_api_success(cursor, rec[\"person_id\"], uuid, timestamp)\n",
    "\n",
    "                        conn.commit()\n",
    "                        break  # Exit retry loop\n",
    "\n",
    "                    else:\n",
    "                        # Response count mismatch\n",
    "                        print(f\"Mismatched response count to sent records: expected {len(batch)}, got {len(response_items)}\")\n",
    "                        for rec in batch:\n",
    "                            update_api_failure(cursor, rec[\"person_id\"], \"Response count mismatch\")\n",
    "                        conn.commit()\n",
    "                        break\n",
    "\n",
    "                else:\n",
    "                    status = resp.status_code\n",
    "                    detail = resp.text\n",
    "\n",
    "                    # Map known statuses to explanation\n",
    "                    msg_map = {\n",
    "                        204: \"No content\",\n",
    "                        400: \"Malformed Payload\",\n",
    "                        401: \"Invalid API token\",\n",
    "                        403: \"API access disallowed\",\n",
    "                        413: \"Payload exceeds limit\",\n",
    "                        429: \"Rate limit exceeded\"\n",
    "                    }\n",
    "\n",
    "                    api_msg = msg_map.get(status, f\"Unexpected Error: {status}\")\n",
    "                    print(f\"API error {status}: {api_msg}\")\n",
    "                    print(\"API response (truncated):\", detail[:250])\n",
    "\n",
    "                    retryable = status in [401, 403, 429]\n",
    "\n",
    "                    if not retryable or retries == max_retries - 1:\n",
    "                        # Final failure\n",
    "                        handle_batch_failure(cursor, batch, status, api_msg, detail)\n",
    "                        conn.commit()\n",
    "                        break\n",
    "                    else:\n",
    "                        # Wait and retry\n",
    "                        print(f\"Retrying in {retry_delay}s (retry {retries + 1}/{max_retries})...\")\n",
    "                        time.sleep(retry_delay)\n",
    "                        retry_delay = min(30, retry_delay * 2)\n",
    "                        retries += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                # Network or unexpected exception\n",
    "                print(f\"Request failed: {e}\")\n",
    "                for rec in batch:\n",
    "                    update_api_failure(cursor, rec[\"person_id\"], str(e))\n",
    "                conn.commit()\n",
    "                break\n",
    "\n",
    "\n",
    "# PEP 484 signature:\n",
    "# def handle_batch_failure(cursor: pyodbc.Cursor, batch: List[Dict[str, Any]], status_code: int, error_message: str, error_detail: str) -> None:\n",
    "@benchmark_section(\"handle_batch_failure()\")  # Performance monitor\n",
    "def handle_batch_failure(cursor, batch, status_code, error_message, error_detail):\n",
    "    \"\"\"\n",
    "    Handle API batch failure by logging error messages per record\n",
    "\n",
    "    Args:\n",
    "        cursor: Active database cursor\n",
    "        batch: List of records submitted in batch\n",
    "        status_code: HTTP status returned by API\n",
    "        error_message: General API error description\n",
    "        error_detail: Raw response detail from API\n",
    "    \"\"\"\n",
    "    # Extract failing record indexes from API response\n",
    "    index_matches = re.findall(r\"\\[(\\d+)\\]\", error_detail)\n",
    "    failed_indexes = set(index_matches)\n",
    "\n",
    "    for i, record in enumerate(batch):\n",
    "        person_id = record[\"person_id\"]\n",
    "\n",
    "        # Assign message based on match to error index\n",
    "        if str(i) in failed_indexes:\n",
    "            msg = f\"API error ({status_code}): {error_message} — {error_detail}\"\n",
    "        else:\n",
    "            msg = f\"API error ({status_code}): {error_message} — Record valid but batch failed\"\n",
    "\n",
    "        update_api_failure(cursor, person_id, msg)\n",
    "        print(f\"Logged API error for person_id {person_id}: {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e26b4572-e881-4324-91db-88efb6106904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_pipeline/test.py\n",
    "\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pyodbc\n",
    "\n",
    "# from .config import (\n",
    "#     SQL_CONN_STR,\n",
    "#     CLIENT_ID,\n",
    "#     CLIENT_SECRET,\n",
    "#     SCOPE,\n",
    "#     TOKEN_ENDPOINT,\n",
    "#     SUPPLIER_KEY,\n",
    "#     API_ENDPOINT_LA,\n",
    "# )\n",
    "\n",
    "# ----------------- helpers -----------------\n",
    "\n",
    "def _print_header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "def _ok(label: str):\n",
    "    print(f\"[OK] {label}\")\n",
    "\n",
    "def _fail(label: str, err):\n",
    "    print(f\"[FAIL] {label}: {err}\")\n",
    "\n",
    "# ----------------- public tests -----------------\n",
    "\n",
    "def test_db_connection():\n",
    "    \"\"\"\n",
    "    Verifies SQL connectivity using configured SQL_CONN_STR\n",
    "    Uses harmless SELECT 1. Does not touch staging tables\n",
    "    Returns True/False\n",
    "    \"\"\"\n",
    "    _print_header(\"Database connectivity (test_db_connection)\")\n",
    "    try:\n",
    "        with pyodbc.connect(SQL_CONN_STR, timeout=5) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"SELECT 1\")\n",
    "                cur.fetchone()\n",
    "        _ok(\"Database connection successful (SELECT 1)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        _fail(\"Database connection failed\", e)\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_schema():\n",
    "    \"\"\"\n",
    "    Checks essential columns exist in staging table\n",
    "    This is *advisory* and does not change data\n",
    "    Returns True/False\n",
    "    \"\"\"\n",
    "    _print_header(\"Schema check (test_schema)\")\n",
    "    table_name = \"ssd_api_data_staging_anon\"\n",
    "    expected = [\"id\", \"json_payload\", \"partial_json_payload\", \"submission_status\"]\n",
    "    recommended = [\"row_state\", \"previous_json_payload\"]\n",
    "\n",
    "    try:\n",
    "        with pyodbc.connect(SQL_CONN_STR, timeout=10) as conn:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(f\"SELECT TOP 0 * FROM {table_name}\")\n",
    "            columns = [column[0] for column in cur.description]\n",
    "\n",
    "        missing = [col for col in expected if col not in columns]\n",
    "        if missing:\n",
    "            print(f\"Missing REQUIRED columns in {table_name}: {', '.join(missing)}\")\n",
    "            return False\n",
    "        _ok(\"Required columns present: \" + \", \".join(expected))\n",
    "\n",
    "        missing_recommended = [col for col in recommended if col not in columns]\n",
    "        if missing_recommended:\n",
    "            print(\"Note: missing recommended columns: \" + \", \".join(missing_recommended))\n",
    "        else:\n",
    "            _ok(\"Recommended columns present: \" + \", \".join(recommended))\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        _fail(\"Schema check failed\", e)\n",
    "        return False\n",
    "\n",
    "\n",
    "# ----------------- optional orchestrator -----------------\n",
    "\n",
    "def run_smoke():\n",
    "    \"\"\"\n",
    "    Composite 'smoke' run that chks:\n",
    "      - DB connectivity (SELECT 1)\n",
    "      - Token acquisition\n",
    "      - Harmless POST to API_ENDPOINT_LA with dummy payload\n",
    "      - Advisory schema check (won't block install if table doesn't exist yet)\n",
    "    Returns True/False and output compact summary, also for CI or scheduled tasks\n",
    "    \"\"\"\n",
    "    _print_header(\"Smoke run (no data required)\")\n",
    "    start = time.time()\n",
    "    db_ok = test_db_connection()\n",
    "    api_ok = _smoke_post_to_api()\n",
    "    schema_ok = test_schema()\n",
    "\n",
    "    overall = db_ok and api_ok and schema_ok\n",
    "    duration = time.time() - start\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"  DB connectivity : {'PASS' if db_ok else 'FAIL'}\")\n",
    "    print(f\"  API POST/Auth   : {'PASS' if api_ok else 'FAIL'}\")\n",
    "    print(f\"  Schema advisory : {'PASS' if schema_ok else 'FAIL'}\")\n",
    "    print(f\"Completed in {duration:.2f}s\")\n",
    "\n",
    "    return overall\n",
    "\n",
    "\n",
    "def _smoke_post_to_api():\n",
    "    \"\"\"\n",
    "    Acquires token and sends a dummy but valid POST to the API.\n",
    "    This tests auth + POST body formatting without using real child data.\n",
    "    \"\"\"\n",
    "    from config import LA_CODE\n",
    "\n",
    "    print(\"Requesting token for smoke test...\")\n",
    "    token_data = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": CLIENT_ID,\n",
    "        \"client_secret\": CLIENT_SECRET,\n",
    "        \"scope\": SCOPE\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(TOKEN_ENDPOINT, data=token_data)\n",
    "        response.raise_for_status()\n",
    "        access_token = response.json().get(\"access_token\")\n",
    "    except Exception as e:\n",
    "        print(\"Token acquisition failed:\", e)\n",
    "        print(\"Response body:\", getattr(e.response, \"text\", \"No response body\"))\n",
    "        return False\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token.strip()}\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\",\n",
    "        \"SupplierKey\": str(SUPPLIER_KEY).strip(),\n",
    "        \"User-Agent\": \"Microsoft PowerShell/6.0.0\"\n",
    "    }\n",
    "\n",
    "\n",
    "    # dummy payload from DfE 0.8.0 spec\n",
    "    dummy_child = {\n",
    "        \"la_child_id\": f\"Child1234{LA_CODE}\",  # make id la specific for tests\n",
    "        \"mis_child_id\": \"Supplier-Child-1234\",\n",
    "        \"child_details\": {\n",
    "            \"unique_pupil_number\": \"ABC0123456789\",\n",
    "            \"former_unique_pupil_number\": \"DEF0123456789\",\n",
    "            \"unique_pupil_number_unknown_reason\": \"UN1\",\n",
    "            \"first_name\": \"John\",\n",
    "            \"surname\": \"Doe\",\n",
    "            \"date_of_birth\": \"2022-06-14\",\n",
    "            \"expected_date_of_birth\": \"2022-06-14\",\n",
    "            \"sex\": \"M\",\n",
    "            \"ethnicity\": \"WBRI\",\n",
    "            \"disabilities\": [\"HAND\", \"VIS\"],\n",
    "            \"postcode\": \"AB12 3DE\",\n",
    "            \"uasc_flag\": True,\n",
    "            \"uasc_end_date\": \"2022-06-14\",\n",
    "            \"purge\": False\n",
    "        },\n",
    "        \"purge\": False\n",
    "    }\n",
    "\n",
    "    dummy_payload = [dummy_child]  # Must be array\n",
    "\n",
    "    print(\"Sending dummy POST to API...\")\n",
    "    try:\n",
    "        response = requests.post(API_ENDPOINT_LA, headers=headers, json=dummy_payload)\n",
    "        print(f\"Status: {response.status_code}\")\n",
    "        if response.status_code in [200, 400, 422]:\n",
    "            print(\"API responded to dummy POST (as expected)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Unexpected API response:\")\n",
    "            print(response.text[:1000])\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(\"POST request failed:\", e)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9721ea4",
   "metadata": {},
   "source": [
    "## Main Controller\n",
    "File: `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2fd315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_pipeline/main.py\n",
    "# core pipeline execution logic: connecting to DB, generating payload, authenticating, and API sends\n",
    "# \n",
    "import pyodbc\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# from .config import SQL_CONN_STR, USE_PARTIAL_PAYLOAD, SUPPLIER_KEY, API_ENDPOINT_LA\n",
    "# from .auth import get_oauth_token\n",
    "# from .db import update_partial_payloads, get_pending_records, update_api_success, update_api_failure\n",
    "# from .api import process_batches\n",
    "# from .utils import benchmark_section, log_debug, announce_mode\n",
    "\n",
    "\n",
    "# # ---- local imports with fallback for notebook/debug use ----\n",
    "# try:\n",
    "#     from .config import SQL_CONN_STR, USE_PARTIAL_PAYLOAD, SUPPLIER_KEY, API_ENDPOINT_LA\n",
    "#     from .auth import get_oauth_token\n",
    "#     from .db import update_partial_payloads, get_pending_records, update_api_success, update_api_failure\n",
    "#     from .api import process_batches\n",
    "#     from .utils import benchmark_section, log_debug, announce_mode\n",
    "# except ImportError:\n",
    "#     from config import SQL_CONN_STR, USE_PARTIAL_PAYLOAD, SUPPLIER_KEY, API_ENDPOINT_LA\n",
    "#     from auth import get_oauth_token\n",
    "#     from db import update_partial_payloads, get_pending_records, update_api_success, update_api_failure\n",
    "#     from api import process_batches\n",
    "#     from utils import benchmark_section, log_debug, announce_mode\n",
    "\n",
    "\n",
    "@benchmark_section(\"main()\")\n",
    "def main():\n",
    "\n",
    "    announce_mode()\n",
    "    log_debug(f\"Connecting to DB using: {SQL_CONN_STR}\")\n",
    "    try:\n",
    "        conn = pyodbc.connect(SQL_CONN_STR, timeout=10)\n",
    "    except Exception as e:\n",
    "        print(f\"Database connection failed: {e}\")\n",
    "        log_debug(f\"Failed to connect DB or timeout occured.\")\n",
    "        return\n",
    "\n",
    "    if USE_PARTIAL_PAYLOAD:\n",
    "        \n",
    "            update_partial_payloads(conn)\n",
    "            log_debug(\"Partial payloads updated.\")\n",
    "        \n",
    "    token = get_oauth_token()\n",
    "    if not token:\n",
    "        print(\"No token retrieved. Exiting.\")\n",
    "        return\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.strip()}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"SupplierKey\": str(SUPPLIER_KEY).strip(),\n",
    "        \"User-Agent\": \"Microsoft PowerShell/6.0.0\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    # DEBUG\n",
    "    # restrict output of secret(s) in console debug|logging    \n",
    "    def format_header_value(key, value, mask_len=5):\n",
    "        \"\"\"Safe, readable debug formatting for headers.\"\"\"\n",
    "        k = str(key).lower()\n",
    "        v = str(value)\n",
    "    \n",
    "        if k == \"authorization\":\n",
    "            # match \"<scheme> <credentials>\" with any spacing, any case\n",
    "            m = re.match(r'^\\s*([A-Za-z]+)\\s+(.+)\\s*$', v)\n",
    "            if m and m.group(1).lower() == \"bearer\":\n",
    "                scheme = \"Bearer\"\n",
    "                token = m.group(2)\n",
    "                return f\"{scheme} {token[:mask_len]}...\"\n",
    "            # if not Bearer or no space, mask first part anyway\n",
    "            return f\"{v[:mask_len]}...\"\n",
    "    \n",
    "        if k == \"supplierkey\":\n",
    "            return f\"{v[:mask_len]}...\"\n",
    "        return v\n",
    "    \n",
    "    # DEBUG\n",
    "    header_preview = \"\\n\".join(f\"    {k}: {format_header_value(k, v)}\" for k, v in headers.items())\n",
    "    log_debug(\"Headers preview:\\n\" + header_preview)\n",
    "    log_debug(f\"API endpoint: {API_ENDPOINT_LA}\")\n",
    "    log_debug(\"\\nFetching pending records from DB...\")\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    records = get_pending_records(cursor)\n",
    "    log_debug(f\"Fetched {len(records)} pending records.\")  \n",
    "\n",
    "    if not records:\n",
    "        print(\"No pending records to send.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Sending {len(records)} records...\")\n",
    "    log_debug(\"Beginning batch API submission...\") \n",
    "    process_batches(records, headers, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6a081",
   "metadata": {},
   "source": [
    "## Optional: Diagnostics | Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38e0b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DIAG] recursive_diff() calls: 0\n",
      "[DIAG] Total time in recursive_diff(): 0.00s\n",
      "\n",
      "[DIAG] prune_unchanged_list() calls: 0\n",
      "[DIAG] Total time in prune_unchanged_list(): 0.00s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Dry run: compare hashes, generate diff report (no submission)\n",
    "# print_diff_stats()\n",
    "\n",
    "# # DB connection test\n",
    "# test_db_connection(SQL_CONN_STR)\n",
    "\n",
    "# # Schema advisory check\n",
    "# test_schema()\n",
    "\n",
    "# Run smoke tests(incl the above) with fake payload\n",
    "run_smoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf436e2f",
   "metadata": {},
   "source": [
    "## Run Submission\n",
    "\n",
    "Uncomment the below to enable the main/full pipeline - by default this set to pull only from the api_data_staging_ANON table(as failsafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b8b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main()...\n",
      "Running in development mode\n",
      "Partial delta payload mode enabled\n",
      "Connecting to DB using: DRIVER={ODBC Driver 17 for SQL Server};SERVER=ESLLREPORTS04V;DATABASE=HDM_Local;Trusted_Connection=yes;\n",
      "Starting update_partial_payloads()...\n",
      "Updated 0 partial_json_payload records\n",
      "[DIAG] Checked: 0\n",
      "[DIAG] Skipped (state): 0\n",
      "[DIAG] Skipped (identical JSON): 0\n",
      "[DIAG] Deleted payloads: 0\n",
      "[DIAG] Delta payloads: 0\n",
      "[DIAG] Errors: 0\n",
      "Finished update_partial_payloads(): 0.23s | Mem delta: 0.03 MiB | Peak mem: 0.15 MiB\n",
      "Partial payloads updated.\n",
      "OAuth token retrieved.\n",
      "TOKEN (first 10 chars): eyJ0eXAiOi...\n",
      "Headers preview:\n",
      "    Authorization: Bearer eyJ0e...\n",
      "    Content-Type: application/json\n",
      "    SupplierKey: 6736a...\n",
      "    User-Agent: Microsoft PowerShell/6.0.0\n",
      "API endpoint: https://pp-api.education.gov.uk/children-in-social-care-data-receiver-test/1/children_social_care_data/845/children\n",
      "\n",
      "Fetching pending records from DB...\n",
      "Starting get_pending_records()...\n",
      "Finished get_pending_records(): 0.23s | Mem delta: 0.00 MiB | Peak mem: 0.15 MiB\n",
      "Fetched 0 pending records.\n",
      "No pending records to send.\n",
      "Finished main(): 1.09s | Mem delta: 4.79 MiB | Peak mem: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
